---
title: "Fireforest"
output: pdf_document
author: Team2-18 Ethan Hu, Hang Su, Ryan Jin, Yang Li
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) #echo:whether to include R source code in the output file
rm(list=ls())
# options(warn=-1)   # Supress warning messages
####################################################
### Functions
####################################################
installIfAbsentAndLoad <- function(neededVector) {
  for(thispackage in neededVector) {
    if( ! require(thispackage, character.only = T) )
    { install.packages(thispackage)}
    require(thispackage, character.only = T)
  }
}
##############################
### Load required packages ###
##############################
# The glmnet package contains functionality for both Ridge
# and Lasso
needed <- c('ISLR', 'glmnet','rpart','rattle','randomForest','e1071','gbm','tidyverse', 'cluster','factoextra','gridExtra','ggplot2')  
installIfAbsentAndLoad(needed)
```
```{r load data and pretreat it, echo=TRUE}
fire <- read.csv('forestfires.csv')
hist(fire$area)
y_engineer <- function(x){
  y <- log(x+1)
  return(y)}
fire$area <- sapply(fire[,ncol(fire)],y_engineer)
hist(fire$area, main = 'Histogram of Transformed Area',xlab = 'Area')
fire <- fire[,-3:-4]
```
```{r data partition}
set.seed(5082)
x <- model.matrix(fire$area~.,fire)[,-1]
x <- scale(x)
y <- fire$area
trainindices <- sample(1:nrow(fire), .80*nrow(fire))
testindices <- setdiff(1:nrow(fire), trainindices)   
trainset <- fire[trainindices,]
testset <- fire[testindices,]

#for y since it is not a dataframe, do not use the indices.
train_x <- x[trainindices,]
train_y <- y[trainindices]
test_x <-x[testindices,]
test_y <- y[testindices]

train.data <- data.frame(train_x, train_y)

```
```{r VIF}
reg <- lm(area~., data = trainset)
car::vif(reg)
```

```{r Unsupervised Learning}
pr.out <- prcomp(train.data, scale=TRUE)
pr.var <- pr.out$sdev^2
pve <- pr.var/sum(pr.var)
biplot(pr.out, scale=0)

plot(pve, xlab="Principal Component", 
     ylab="Proportion of Variance Explained", 
     ylim=c(0,1),type='b')
plot(cumsum(pve), xlab="Principal Component", 
     ylab="Cumulative Proportion of Variance Explained", 
     ylim=c(0,1),type='b')
```

```{r Lasso Ridge}
grid <- 10 ^ seq(10, -3, length=100)
ridge.mod <- glmnet(train_x, 
                    train_y, 
                    alpha=0, 
                    lambda=grid)

cv.out.class.r = cv.glmnet(train_x, 
                           train_y,
                           alpha=0, 
                           lambda=grid)

plot(cv.out.class.r)

bestlam.r <- cv.out.class.r$lambda.min
bestlam.r

ridge.pred.class <- predict(ridge.mod, 
                            s=bestlam.r, 
                            newx=test_x)

(error_rate1 <- mean((ridge.pred.class-test_y)^2))

(ridege.coefficients <- predict(ridge.mod, 
                                s=bestlam.r, 
                                newx=test_x,
                                type="coefficients"))

mode.lasso <- glmnet(train_x, 
                     train_y, 
                     alpha=1, 
                     lambda=grid)

cv.out.class.r.lasso = cv.glmnet(train_x, 
                                 train_y,
                                 alpha=1, 
                                 lambda=grid)

plot(cv.out.class.r.lasso)

bestlam.r.lasso <- cv.out.class.r.lasso$lambda.min
bestlam.r.lasso

lasso.pred.class <- predict(mode.lasso, 
                            s=bestlam.r.lasso, 
                            newx=test_x)

(error_rate2 <- mean((lasso.pred.class-test_y)^2))

(lasso.coefficients <- predict(mode.lasso, 
                               s=bestlam.r.lasso, 
                               newx=test_x,
                               type="coefficients"))
```

```{r SVM, warning=FALSE}
SVMmodel <- svm(train_y ~ train_x , train.data)

tuneResultlinear <- tune(svm, train_y ~ train_x, kernel = "linear",type = "eps-regression",
                         ranges = list( cost = 2^(2:9)))
print(tuneResultlinear)
plot(tuneResultlinear)

tunedModellinear <- tuneResultlinear$best.model
tunedModellinear
PredictModelYlinear <- predict(tunedModellinear, newx = test_x)

error.linear <- (train.data$train_y - PredictModelYlinear)^2
(svrPredictionRMSElinear <- mean(error.linear))

tuneResultradial <- tune(svm, train_y ~ train_x, kernel = "radial",type = "eps-regression",
                         ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9)))

print(tuneResultradial)
plot(tuneResultradial)

tunedModel <- tuneResultradial$best.model
tunedModel
PredictModelY <- predict(tunedModel, newx = test_x)

error <- (train.data$train_y - PredictModelY)^2
(error_rate3 <- svrPredictionRMSE <- mean(error))
```

```{r regression tree}
rpart.fit <- rpart(area ~ .,trainset, method = "anova", parms = list(split = "information"), 
                   control = rpart.control(minsplit=20, minbucket=7, cp=0.01))
xerr <- rpart.fit$cptable[,"xerror"]
#we pick the one with least standard error
minxerr <- which.min(xerr)
mincp <- rpart.fit$cptable[minxerr,"CP"]
rpart.prune <- prune(rpart.fit,cp = mincp)

plot(rpart.fit)
text(rpart.fit,pretty=0)
fancyRpartPlot(rpart.fit, main="Fancy Tree")

yhat <- predict(rpart.prune,testset)

#Calculate MSE
(error_rate4 <- (mean((yhat-testset$area)^2))) #1.745
```


```{r}
min.mse<-rep(0,10) # double(10)/ out of bag error
min.ntree<-rep(0,10) #test error
for(mtry in 1:10){
  rf_mtry <- randomForest(formula=area ~ .,data=trainset, ntree=500, mtry=mtry,
                          importance=TRUE,localImp=TRUE,na.action=na.roughfix,
                          replace=TRUE)
  min.mse[mtry] <- rf_mtry$mse[500]
  pred<-predict(rf_mtry,testset)
  min.ntree[mtry] <-  with(testset, mean( (area - pred)^2))
  cat(mtry," ")
}
#Plot the columns of one matrix against the columns of another.
matplot(1:mtry,cbind(min.mse, min.ntree),pch=19,col=c("red","blue"), type="b",ylab="mse",xlab = "Number of Predictors Considered at each split")
legend("topright",legend=c("MSE","Test Error"),pch=19, col=c("red","blue"))

rf <- randomForest(area ~ .,data=trainset,ntree=500,
                   mtry=which.min(min.ntree),
                   importance=TRUE, na.action=na.roughfix,replace=TRUE)
plot(rf)
varImpPlot(rf)
importance(rf)
rf.test<-predict(rf,newdata = testset)
rf.resid<-rf.test-testset$area
(error_rate5 <- mean(rf.resid^2))
```

```{r}
model <- c('ridge','lasso','svm','classification_tree', 'random_forest')
vec_error <- c(error_rate1,error_rate2,error_rate3, error_rate4,error_rate5)
#a <- do.call(rbind.data.frame, Map('c', model, vec_error))
a <- data.frame(Model = model,Error_rate=vec_error)
print(a)
print(paste('The lowest error rate among all four models in steps e to j is ',vec_error[which.min(vec_error)]))
print(paste('The best model is ',as.character(a$Model[[which.min(vec_error)]])))
```









































